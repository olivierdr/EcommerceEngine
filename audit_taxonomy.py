"""
√âtape 1 - Audit de la taxonomie existante

Ce script analyse le jeu de donn√©es d'entra√Ænement pour :
1. Comprendre la structure de la taxonomie (profondeur, nombre de cat√©gories par niveau)
2. D√©tecter des incoh√©rences structurelles dans les category_path
3. √âvaluer la coh√©rence s√©mantique des produits au sein de chaque cat√©gorie
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# Pour les embeddings s√©mantiques
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("‚ö†Ô∏è  sentence-transformers non install√©. L'analyse s√©mantique sera limit√©e.")


class TaxonomyAuditor:
    """Auditeur de taxonomie pour produits e-commerce"""
    
    def __init__(self, data_path, sample_size=None):
        """
        Parameters:
        -----------
        data_path : str
            Chemin vers le fichier CSV d'entra√Ænement
        sample_size : int, optional
            Nombre d'√©chantillons √† utiliser pour l'analyse s√©mantique (None = tout)
        """
        self.data_path = data_path
        self.sample_size = sample_size
        self.df = None
        self.embedding_model = None
        
    def load_data(self):
        """Charge les donn√©es d'entra√Ænement"""
        print("üìä Chargement des donn√©es...")
        self.df = pd.read_csv(self.data_path)
        print(f"   ‚úì {len(self.df):,} produits charg√©s")
        return self.df
    
    def analyze_structure(self):
        """Analyse la structure de la taxonomie"""
        print("\n" + "="*60)
        print("1Ô∏è‚É£  ANALYSE DE LA STRUCTURE DE LA TAXONOMIE")
        print("="*60)
        
        # Profondeur des chemins
        self.df['path_depth'] = self.df['category_path'].str.count('/') + 1
        depths = self.df['path_depth'].value_counts().sort_index()
        
        print(f"\nüìè Profondeur de la taxonomie:")
        print(f"   Profondeur minimale: {depths.index.min()}")
        print(f"   Profondeur maximale: {depths.index.max()}")
        print(f"   Profondeur moyenne: {self.df['path_depth'].mean():.2f}")
        print(f"   Profondeur m√©diane: {self.df['path_depth'].median():.0f}")
        
        print(f"\nüìä Distribution des profondeurs:")
        for depth, count in depths.items():
            pct = (count / len(self.df)) * 100
            print(f"   Niveau {depth}: {count:,} produits ({pct:.1f}%)")
        
        # Nombre de cat√©gories uniques par niveau
        print(f"\nüè∑Ô∏è  Nombre de cat√©gories uniques par niveau:")
        max_depth = int(self.df['path_depth'].max())
        for level in range(1, max_depth + 1):
            categories_at_level = set()
            for path in self.df['category_path']:
                parts = path.split('/')
                if len(parts) >= level:
                    categories_at_level.add(parts[level - 1])
            print(f"   Niveau {level}: {len(categories_at_level):,} cat√©gories uniques")
        
        # Cat√©gories feuilles
        unique_leaf_categories = self.df['category_id'].nunique()
        print(f"\nüçÉ Cat√©gories feuilles (category_id):")
        print(f"   {unique_leaf_categories:,} cat√©gories feuilles uniques")
        print(f"   {len(self.df):,} produits au total")
        avg_products_per_category = len(self.df) / unique_leaf_categories
        print(f"   Moyenne: {avg_products_per_category:.1f} produits par cat√©gorie")
        
        # Distribution des produits par cat√©gorie
        category_counts = self.df['category_id'].value_counts()
        print(f"\nüìà Distribution des produits par cat√©gorie:")
        print(f"   Cat√©gorie la plus fr√©quente: {category_counts.max():,} produits")
        print(f"   Cat√©gorie la moins fr√©quente: {category_counts.min():,} produits")
        print(f"   M√©diane: {category_counts.median():.0f} produits")
        
        # Top 5 cat√©gories les plus fr√©quentes
        print(f"\nüèÜ Top 5 cat√©gories les plus fr√©quentes:")
        top_5 = category_counts.head(5)
        for cat_id, count in top_5.items():
            cat_products = self.df[self.df['category_id'] == cat_id]
            example_title = cat_products.iloc[0]['title'] if len(cat_products) > 0 else "N/A"
            example_title = example_title[:60] + "..." if len(str(example_title)) > 60 else example_title
            print(f"   {cat_id}: {count:,} produits | Ex: {example_title}")
        
        # Bottom 5 cat√©gories les moins fr√©quentes
        print(f"\nüìâ Top 5 cat√©gories les moins fr√©quentes:")
        bottom_5 = category_counts.tail(5)
        for cat_id, count in bottom_5.items():
            cat_products = self.df[self.df['category_id'] == cat_id]
            example_title = cat_products.iloc[0]['title'] if len(cat_products) > 0 else "N/A"
            example_title = example_title[:60] + "..." if len(str(example_title)) > 60 else example_title
            print(f"   {cat_id}: {count:,} produits | Ex: {example_title}")
        
        # Cat√©gories avec peu de produits (potentiellement probl√©matiques)
        rare_categories = (category_counts < 5).sum()
        print(f"\n‚ö†Ô∏è  Cat√©gories rares (< 5 produits): {rare_categories:,} ({rare_categories/unique_leaf_categories*100:.1f}%)")
        
        return {
            'depths': depths,
            'max_depth': max_depth,
            'unique_leaf_categories': unique_leaf_categories,
            'category_counts': category_counts
        }
    
    def detect_inconsistencies(self):
        """D√©tecte les incoh√©rences structurelles dans les category_path"""
        print("\n" + "="*60)
        print("2Ô∏è‚É£  D√âTECTION D'INCOH√âRENCES STRUCTURELLES")
        print("="*60)
        
        inconsistencies = {
            'category_id_mismatch': [],
            'empty_paths': [],
            'invalid_paths': []
        }
        
        # V√©rifier que category_id correspond au dernier √©l√©ment du path
        print("\nüîç V√©rification de coh√©rence category_id / category_path...")
        mismatches = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            last_path_id = path_parts[-1] if path_parts else None
            if last_path_id != row['category_id']:
                mismatches += 1
                inconsistencies['category_id_mismatch'].append({
                    'product_id': row['product_id'],
                    'category_id': row['category_id'],
                    'last_path_id': last_path_id,
                    'category_path': row['category_path']
                })
        
        if mismatches > 0:
            print(f"   ‚ö†Ô∏è  {mismatches:,} incoh√©rences d√©tect√©es (category_id ‚â† dernier √©l√©ment du path)")
            print(f"   Exemples (premiers 5):")
            for inc in inconsistencies['category_id_mismatch'][:5]:
                print(f"      - Product: {inc['product_id'][:8]}... | category_id: {inc['category_id']} | path fin: {inc['last_path_id']}")
        else:
            print(f"   ‚úì Aucune incoh√©rence d√©tect√©e")
        
        # V√©rifier les paths vides ou invalides
        print("\nüîç V√©rification des paths vides ou invalides...")
        empty_paths = self.df[self.df['category_path'].isna() | (self.df['category_path'] == '')]
        if len(empty_paths) > 0:
            print(f"   ‚ö†Ô∏è  {len(empty_paths):,} produits avec path vide")
            inconsistencies['empty_paths'] = empty_paths['product_id'].tolist()
        else:
            print(f"   ‚úì Aucun path vide")
        
        # V√©rifier les paths avec des IDs invalides (format hexad√©cimal attendu)
        print("\nüîç V√©rification du format des IDs dans les paths...")
        invalid_format = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for part in path_parts:
                # V√©rifier que c'est un hexad√©cimal de 8 caract√®res
                if len(part) != 8 or not all(c in '0123456789abcdef' for c in part.lower()):
                    invalid_format += 1
                    inconsistencies['invalid_paths'].append({
                        'product_id': row['product_id'],
                        'category_path': row['category_path'],
                        'invalid_part': part
                    })
                    break
        
        if invalid_format > 0:
            print(f"   ‚ö†Ô∏è  {invalid_format:,} paths avec format d'ID invalide")
        else:
            print(f"   ‚úì Tous les IDs ont un format valide (8 caract√®res hex)")
        
        # V√©rifier les chemins avec des doublons cons√©cutifs
        print("\nüîç V√©rification des doublons cons√©cutifs dans les paths...")
        consecutive_duplicates = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for i in range(len(path_parts) - 1):
                if path_parts[i] == path_parts[i + 1]:
                    consecutive_duplicates += 1
                    break
        
        if consecutive_duplicates > 0:
            print(f"   ‚ö†Ô∏è  {consecutive_duplicates:,} paths avec doublons cons√©cutifs")
        else:
            print(f"   ‚úì Aucun doublon cons√©cutif")
        
        return inconsistencies
    
    def save_rare_categories(self, threshold=10, output_file='rare_categories.json'):
        """
        Sauvegarde les cat√©gories rares dans un fichier JSON
        
        Parameters:
        -----------
        threshold : int
            Seuil en dessous duquel une cat√©gorie est consid√©r√©e comme rare
        output_file : str
            Nom du fichier JSON de sortie
        """
        category_counts = self.df['category_id'].value_counts()
        rare_categories = category_counts[category_counts < threshold]
        
        rare_data = []
        for cat_id, count in rare_categories.items():
            cat_products = self.df[self.df['category_id'] == cat_id]
            category_path = cat_products.iloc[0]['category_path'] if len(cat_products) > 0 else ""
            
            rare_data.append({
                'category_id': cat_id,
                'category_path': category_path,
                'n_products': int(count)
            })
        
        # Trier par nombre de produits (croissant)
        rare_data.sort(key=lambda x: x['n_products'])
        
        output_path = Path(__file__).parent / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': threshold,
                'total_rare_categories': len(rare_data),
                'categories': rare_data
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Cat√©gories rares sauvegard√©es: {output_file}")
        print(f"   Seuil: < {threshold} produits")
        print(f"   {len(rare_data)} cat√©gories trouv√©es")
        
        return rare_data
    
    def save_low_coherence_categories(self, semantic_results, threshold=0.4, output_file='low_coherence_categories.json'):
        """
        Sauvegarde les cat√©gories √† faible coh√©rence s√©mantique dans un fichier JSON
        
        Parameters:
        -----------
        semantic_results : pd.DataFrame
            R√©sultats de l'analyse de coh√©rence s√©mantique
        threshold : float
            Seuil en dessous duquel une cat√©gorie est consid√©r√©e comme peu coh√©rente
        output_file : str
            Nom du fichier JSON de sortie
        """
        if semantic_results is None:
            print("\n‚ö†Ô∏è  Impossible de sauvegarder: analyse s√©mantique non disponible")
            return None
        
        low_coherence = semantic_results[semantic_results['coherence_score'] < threshold].copy()
        
        low_coherence_data = []
        for _, row in low_coherence.iterrows():
            cat_id = row['category_id']
            cat_products = self.df[self.df['category_id'] == cat_id]
            category_path = cat_products.iloc[0]['category_path'] if len(cat_products) > 0 else ""
            
            low_coherence_data.append({
                'category_id': cat_id,
                'category_path': category_path,
                'n_products': int(row['n_products']),
                'coherence_score': float(row['coherence_score']),
                'avg_semantic_distance': float(row['avg_semantic_distance'])
            })
        
        # Trier par score de coh√©rence (croissant)
        low_coherence_data.sort(key=lambda x: x['coherence_score'])
        
        output_path = Path(__file__).parent / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': threshold,
                'total_low_coherence_categories': len(low_coherence_data),
                'categories': low_coherence_data
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Cat√©gories √† faible coh√©rence sauvegard√©es: {output_file}")
        print(f"   Seuil: < {threshold}")
        print(f"   {len(low_coherence_data)} cat√©gories trouv√©es")
        
        return low_coherence_data
    
    def evaluate_semantic_coherence(self, top_n_categories=20, min_products=10, analyze_all=False):
        """
        √âvalue la coh√©rence s√©mantique des produits au sein de chaque cat√©gorie
        
        Parameters:
        -----------
        top_n_categories : int
            Nombre de cat√©gories √† analyser en d√©tail (si analyze_all=False)
        min_products : int
            Nombre minimum de produits requis pour analyser une cat√©gorie
        analyze_all : bool
            Si True, analyse toutes les cat√©gories (peut √™tre long)
        """
        print("\n" + "="*60)
        print("3Ô∏è‚É£  √âVALUATION DE LA COH√âRENCE S√âMANTIQUE")
        print("="*60)
        
        if not EMBEDDINGS_AVAILABLE:
            print("\n‚ö†Ô∏è  sentence-transformers non disponible. Analyse s√©mantique limit√©e.")
            print("   Installez avec: pip install sentence-transformers")
            return None
        
        print("\nüîÑ Chargement du mod√®le d'embeddings multilingue...")
        try:
            # Mod√®le multilingue pour FR/DE/EN
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("   ‚úì Mod√®le charg√©")
        except Exception as e:
            print(f"   ‚ùå Erreur lors du chargement: {e}")
            return None
        
        # Pr√©parer les textes (title + description)
        print("\nüìù Pr√©paration des textes produits...")
        self.df['text'] = (
            self.df['title'].fillna('') + ' ' + 
            self.df['description'].fillna('')
        ).str.strip()
        
        # Filtrer les cat√©gories avec suffisamment de produits
        category_counts = self.df['category_id'].value_counts()
        if analyze_all:
            valid_categories = category_counts[category_counts >= min_products].index
            print(f"\nüîç Analyse de toutes les cat√©gories (‚â•{min_products} produits)...")
        else:
            valid_categories = category_counts[category_counts >= min_products].index[:top_n_categories]
            print(f"\nüîç Analyse de {len(valid_categories)} cat√©gories (‚â•{min_products} produits)...")
        
        results = []
        
        for cat_id in valid_categories:
            cat_products = self.df[self.df['category_id'] == cat_id]
            
            # √âchantillonner si trop de produits
            if len(cat_products) > 100:
                cat_products = cat_products.sample(n=100, random_state=42)
            
            texts = cat_products['text'].tolist()
            
            # Calculer les embeddings
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            
            # Calculer la coh√©rence (distance moyenne intra-classe)
            # Plus la distance est faible, plus la cat√©gorie est coh√©rente
            from sklearn.metrics.pairwise import cosine_distances
            distances = cosine_distances(embeddings)
            # Distance moyenne (excluant la diagonale)
            np.fill_diagonal(distances, np.nan)
            avg_distance = np.nanmean(distances)
            
            results.append({
                'category_id': cat_id,
                'n_products': len(cat_products),
                'avg_semantic_distance': avg_distance,
                'coherence_score': 1 - avg_distance  # Plus proche de 1 = plus coh√©rent
            })
        
        results_df = pd.DataFrame(results).sort_values('coherence_score')
        
        print(f"\nüìä R√©sultats de coh√©rence s√©mantique:")
        print(f"   Score moyen: {results_df['coherence_score'].mean():.3f}")
        print(f"   Score m√©dian: {results_df['coherence_score'].median():.3f}")
        print(f"   Score min: {results_df['coherence_score'].min():.3f}")
        print(f"   Score max: {results_df['coherence_score'].max():.3f}")
        
        print(f"\nüèÜ Top 5 cat√©gories les plus coh√©rentes:")
        for _, row in results_df.tail(5).iterrows():
            print(f"   {row['category_id']}: {row['coherence_score']:.3f} ({row['n_products']} produits)")
        
        print(f"\n‚ö†Ô∏è  Top 5 cat√©gories les moins coh√©rentes (potentiellement bruit√©es):")
        for _, row in results_df.head(5).iterrows():
            print(f"   {row['category_id']}: {row['coherence_score']:.3f} ({row['n_products']} produits)")
        
        return results_df
    
    def generate_report(self):
        """G√©n√®re un rapport complet d'audit"""
        print("\n" + "="*60)
        print("üìã RAPPORT D'AUDIT COMPLET")
        print("="*60)
        
        # Charger les donn√©es
        self.load_data()
        
        # Analyses
        structure_info = self.analyze_structure()
        inconsistencies = self.detect_inconsistencies()
        # Analyser toutes les cat√©gories pour avoir des r√©sultats complets
        semantic_results = self.evaluate_semantic_coherence(analyze_all=True)
        
        # Sauvegarder les cat√©gories rares (seuil: < 10 produits)
        self.save_rare_categories(threshold=10)
        
        # Sauvegarder les cat√©gories √† faible coh√©rence (seuil: < 0.4)
        if semantic_results is not None:
            self.save_low_coherence_categories(semantic_results, threshold=0.4)
        
        # R√©sum√©
        print("\n" + "="*60)
        print("üìå R√âSUM√â")
        print("="*60)
        
        total_issues = (
            len(inconsistencies['category_id_mismatch']) +
            len(inconsistencies['empty_paths']) +
            len(inconsistencies['invalid_paths'])
        )
        
        print(f"\n‚úÖ Points positifs:")
        print(f"   - {structure_info['unique_leaf_categories']:,} cat√©gories feuilles identifi√©es")
        print(f"   - Profondeur maximale: {structure_info['max_depth']} niveaux")
        
        if total_issues > 0:
            print(f"\n‚ö†Ô∏è  Points d'attention:")
            print(f"   - {total_issues:,} incoh√©rences structurelles d√©tect√©es")
            if len(inconsistencies['category_id_mismatch']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['category_id_mismatch'])} mismatches category_id/path")
            if len(inconsistencies['empty_paths']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['empty_paths'])} paths vides")
            if len(inconsistencies['invalid_paths']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['invalid_paths'])} paths avec format invalide")
        else:
            print(f"\n‚úÖ Aucune incoh√©rence structurelle majeure d√©tect√©e")
        
        if semantic_results is not None:
            low_coherence = (semantic_results['coherence_score'] < 0.3).sum()
            if low_coherence > 0:
                print(f"   - {low_coherence} cat√©gories avec faible coh√©rence s√©mantique (< 0.3)")
        
        print("\n" + "="*60)
        print("‚úì Audit termin√©")
        print("="*60)


def main():
    """Point d'entr√©e principal"""
    data_path = Path(__file__).parent / 'data' / 'trainset.csv'
    
    if not data_path.exists():
        print(f"‚ùå Fichier non trouv√©: {data_path}")
        return
    
    auditor = TaxonomyAuditor(data_path)
    auditor.generate_report()


if __name__ == '__main__':
    main()

