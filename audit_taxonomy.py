"""
√âtape 1 - Audit de la taxonomie existante

Ce script analyse le jeu de donn√©es d'entra√Ænement pour :
1. Comprendre la structure de la taxonomie (profondeur, nombre de cat√©gories par niveau)
2. D√©tecter des incoh√©rences structurelles dans les category_path
3. √âvaluer la coh√©rence s√©mantique des produits au sein de chaque cat√©gorie
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from pathlib import Path
import json
import re
import warnings
warnings.filterwarnings('ignore')

# Pour les embeddings s√©mantiques
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("‚ö†Ô∏è  sentence-transformers non install√©. L'analyse s√©mantique sera limit√©e.")


class TaxonomyAuditor:
    """Auditeur de taxonomie pour produits e-commerce"""
    
    def __init__(self, data_path, sample_size=None):
        """
        Parameters:
        -----------
        data_path : str
            Chemin vers le fichier CSV d'entra√Ænement
        sample_size : int, optional
            Nombre d'√©chantillons √† utiliser pour l'analyse s√©mantique (None = tout)
        """
        self.data_path = data_path
        self.sample_size = sample_size
        self.df = None
        self.embedding_model = None
        
    def load_data(self):
        """Charge les donn√©es d'entra√Ænement"""
        print("üìä Chargement des donn√©es...")
        self.df = pd.read_csv(self.data_path)
        print(f"   ‚úì {len(self.df):,} produits charg√©s")
        return self.df
    
    def analyze_structure(self):
        """Analyse la structure de la taxonomie"""
        print("\n" + "="*60)
        print("1Ô∏è‚É£  ANALYSE DE LA STRUCTURE DE LA TAXONOMIE")
        print("="*60)
        
        # Profondeur des chemins
        self.df['path_depth'] = self.df['category_path'].str.count('/') + 1
        depths = self.df['path_depth'].value_counts().sort_index()
        
        print(f"\nüìè Profondeur de la taxonomie:")
        print(f"   Profondeur minimale: {depths.index.min()}")
        print(f"   Profondeur maximale: {depths.index.max()}")
        print(f"   Profondeur moyenne: {self.df['path_depth'].mean():.2f}")
        print(f"   Profondeur m√©diane: {self.df['path_depth'].median():.0f}")
        
        print(f"\nüìä Distribution des profondeurs:")
        for depth, count in depths.items():
            pct = (count / len(self.df)) * 100
            print(f"   Niveau {depth}: {count:,} produits ({pct:.1f}%)")
        
        # Nombre de cat√©gories uniques par niveau
        print(f"\nüè∑Ô∏è  Nombre de cat√©gories uniques par niveau:")
        max_depth = int(self.df['path_depth'].max())
        for level in range(1, max_depth + 1):
            categories_at_level = set()
            for path in self.df['category_path']:
                parts = path.split('/')
                if len(parts) >= level:
                    categories_at_level.add(parts[level - 1])
            print(f"   Niveau {level}: {len(categories_at_level):,} cat√©gories uniques")
        
        # Cat√©gories feuilles
        unique_leaf_categories = self.df['category_id'].nunique()
        print(f"\nüçÉ Cat√©gories feuilles (category_id):")
        print(f"   {unique_leaf_categories:,} cat√©gories feuilles uniques")
        print(f"   {len(self.df):,} produits au total")
        avg_products_per_category = len(self.df) / unique_leaf_categories
        print(f"   Moyenne: {avg_products_per_category:.1f} produits par cat√©gorie")
        
        # Distribution des produits par cat√©gorie
        category_counts = self.df['category_id'].value_counts()
        print(f"\nüìà Distribution des produits par cat√©gorie:")
        print(f"   Cat√©gorie la plus fr√©quente: {category_counts.max():,} produits")
        print(f"   Cat√©gorie la moins fr√©quente: {category_counts.min():,} produits")
        print(f"   M√©diane: {category_counts.median():.0f} produits")
        
        # Top 5 cat√©gories les plus fr√©quentes
        print(f"\nüèÜ Top 5 cat√©gories les plus fr√©quentes:")
        top_5 = category_counts.head(5)
        for cat_id, count in top_5.items():
            cat_products = self.df[self.df['category_id'] == cat_id]
            example_title = cat_products.iloc[0]['title'] if len(cat_products) > 0 else "N/A"
            example_title = example_title[:60] + "..." if len(str(example_title)) > 60 else example_title
            print(f"   {cat_id}: {count:,} produits | Ex: {example_title}")
        
        # Bottom 5 cat√©gories les moins fr√©quentes
        print(f"\nüìâ Top 5 cat√©gories les moins fr√©quentes:")
        bottom_5 = category_counts.tail(5)
        for cat_id, count in bottom_5.items():
            cat_products = self.df[self.df['category_id'] == cat_id]
            example_title = cat_products.iloc[0]['title'] if len(cat_products) > 0 else "N/A"
            example_title = example_title[:60] + "..." if len(str(example_title)) > 60 else example_title
            print(f"   {cat_id}: {count:,} produits | Ex: {example_title}")
        
        # Cat√©gories avec peu de produits (potentiellement probl√©matiques)
        rare_categories = (category_counts < 5).sum()
        print(f"\n‚ö†Ô∏è  Cat√©gories rares (< 5 produits): {rare_categories:,} ({rare_categories/unique_leaf_categories*100:.1f}%)")
        
        # G√©n√©rer les noms de cat√©gories
        category_names = self.generate_category_names()
        
        return {
            'depths': depths,
            'max_depth': max_depth,
            'unique_leaf_categories': unique_leaf_categories,
            'category_counts': category_counts,
            'category_names': category_names
        }
    
    def generate_category_names(self):
        """G√©n√®re des noms simples pour chaque cat√©gorie bas√©s sur les mots-cl√©s fr√©quents"""
        print("\nüè∑Ô∏è  G√©n√©ration des noms de cat√©gories...")
        
        # Stopwords simples (FR/DE/EN)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'et', 'ou', 'pour', 'avec', 'sans', 
                    'der', 'die', 'das', 'und', 'oder', 'f√ºr', 'mit', 'ohne',
                    'the', 'a', 'an', 'and', 'or', 'for', 'with', 'without',
                    '√†', 'd', 'l', 'un', 'une', 'en', 'sur', 'par', 'dans'}
        
        category_names = {}
        
        for cat_id in self.df['category_id'].unique():
            cat_products = self.df[self.df['category_id'] == cat_id]
            titles = cat_products['title'].fillna('').astype(str).tolist()
            
            # Extraire les mots (min 3 caract√®res, alphanum√©riques)
            words = []
            for title in titles:
                title_words = re.findall(r'\b[a-zA-Z√Ä-√ø]{3,}\b', title.lower())
                words.extend([w for w in title_words if w not in stopwords])
            
            # Top 2-3 mots les plus fr√©quents
            if words:
                word_counts = Counter(words)
                top_words = [word for word, _ in word_counts.most_common(3)]
                category_name = ' '.join(top_words).title()
                category_names[cat_id] = category_name
            else:
                category_names[cat_id] = "Cat√©gorie inconnue"
        
        # Sauvegarder
        output_path = Path(__file__).parent / 'category_names.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(category_names, f, indent=2, ensure_ascii=False)
        
        print(f"   ‚úì {len(category_names)} noms g√©n√©r√©s")
        print(f"   üíæ Sauvegard√©s dans: {output_path}")
        
        # Afficher quelques exemples
        print(f"\n   Exemples de noms g√©n√©r√©s:")
        for i, (cat_id, name) in enumerate(list(category_names.items())[:5]):
            print(f"   {cat_id}: {name}")
        
        return category_names
    
    def detect_inconsistencies(self):
        """D√©tecte les incoh√©rences structurelles dans les category_path"""
        print("\n" + "="*60)
        print("2Ô∏è‚É£  D√âTECTION D'INCOH√âRENCES STRUCTURELLES")
        print("="*60)
        
        inconsistencies = {
            'category_id_mismatch': [],
            'empty_paths': [],
            'invalid_paths': []
        }
        
        # V√©rifier que category_id correspond au dernier √©l√©ment du path
        print("\nüîç V√©rification de coh√©rence category_id / category_path...")
        mismatches = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            last_path_id = path_parts[-1] if path_parts else None
            if last_path_id != row['category_id']:
                mismatches += 1
                inconsistencies['category_id_mismatch'].append({
                    'product_id': row['product_id'],
                    'category_id': row['category_id'],
                    'last_path_id': last_path_id,
                    'category_path': row['category_path']
                })
        
        if mismatches > 0:
            print(f"   ‚ö†Ô∏è  {mismatches:,} incoh√©rences d√©tect√©es (category_id ‚â† dernier √©l√©ment du path)")
            print(f"   Exemples (premiers 5):")
            for inc in inconsistencies['category_id_mismatch'][:5]:
                print(f"      - Product: {inc['product_id'][:8]}... | category_id: {inc['category_id']} | path fin: {inc['last_path_id']}")
        else:
            print(f"   ‚úì Aucune incoh√©rence d√©tect√©e")
        
        # V√©rifier les paths vides ou invalides
        print("\nüîç V√©rification des paths vides ou invalides...")
        empty_paths = self.df[self.df['category_path'].isna() | (self.df['category_path'] == '')]
        if len(empty_paths) > 0:
            print(f"   ‚ö†Ô∏è  {len(empty_paths):,} produits avec path vide")
            inconsistencies['empty_paths'] = empty_paths['product_id'].tolist()
        else:
            print(f"   ‚úì Aucun path vide")
        
        # V√©rifier les paths avec des IDs invalides (format hexad√©cimal attendu)
        print("\nüîç V√©rification du format des IDs dans les paths...")
        invalid_format = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for part in path_parts:
                # V√©rifier que c'est un hexad√©cimal de 8 caract√®res
                if len(part) != 8 or not all(c in '0123456789abcdef' for c in part.lower()):
                    invalid_format += 1
                    inconsistencies['invalid_paths'].append({
                        'product_id': row['product_id'],
                        'category_path': row['category_path'],
                        'invalid_part': part
                    })
                    break
        
        if invalid_format > 0:
            print(f"   ‚ö†Ô∏è  {invalid_format:,} paths avec format d'ID invalide")
        else:
            print(f"   ‚úì Tous les IDs ont un format valide (8 caract√®res hex)")
        
        # V√©rifier les chemins avec des doublons cons√©cutifs
        print("\nüîç V√©rification des doublons cons√©cutifs dans les paths...")
        consecutive_duplicates = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for i in range(len(path_parts) - 1):
                if path_parts[i] == path_parts[i + 1]:
                    consecutive_duplicates += 1
                    break
        
        if consecutive_duplicates > 0:
            print(f"   ‚ö†Ô∏è  {consecutive_duplicates:,} paths avec doublons cons√©cutifs")
        else:
            print(f"   ‚úì Aucun doublon cons√©cutif")
        
        return inconsistencies
    
    def evaluate_semantic_coherence(self, threshold=0.4, min_products=10):
        """Analyse s√©mantique simplifi√©e : √©value la coh√©rence et sauvegarde les cat√©gories probl√©matiques"""
        print("\n" + "="*60)
        print("3Ô∏è‚É£  √âVALUATION DE LA COH√âRENCE S√âMANTIQUE")
        print("="*60)
        
        if not EMBEDDINGS_AVAILABLE:
            print("\n‚ö†Ô∏è  sentence-transformers non disponible.")
            return None
        
        print("\nüîÑ Chargement du mod√®le d'embeddings...")
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Analyser toutes les cat√©gories avec suffisamment de produits
        category_counts = self.df['category_id'].value_counts()
        valid_categories = category_counts[category_counts >= min_products].index
        
        print(f"\nüîç Analyse de {len(valid_categories)} cat√©gories...")
        
        low_coherence_data = []
        texts_combined = (self.df['title'].fillna('') + ' ' + self.df['description'].fillna('')).str.strip()
        
        for cat_id in valid_categories:
            cat_products = self.df[self.df['category_id'] == cat_id]
            if len(cat_products) > 100:
                cat_products = cat_products.sample(n=100, random_state=42)
            
            texts = texts_combined[cat_products.index].tolist()
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            
            # Distance moyenne intra-classe
            from sklearn.metrics.pairwise import cosine_distances
            distances = cosine_distances(embeddings)
            np.fill_diagonal(distances, np.nan)
            avg_distance = np.nanmean(distances)
            coherence_score = 1 - avg_distance
            
            if coherence_score < threshold:
                low_coherence_data.append({
                    'category_id': cat_id,
                    'category_path': cat_products.iloc[0]['category_path'],
                    'n_products': len(cat_products),
                    'coherence_score': float(coherence_score)
                })
        
        # Sauvegarder
        low_coherence_data.sort(key=lambda x: x['coherence_score'])
        output_path = Path(__file__).parent / 'low_coherence_categories.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': threshold,
                'total_low_coherence_categories': len(low_coherence_data),
                'categories': low_coherence_data
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ {len(low_coherence_data)} cat√©gories √† faible coh√©rence sauvegard√©es")
        if low_coherence_data:
            print(f"   Top 3: {', '.join([c['category_id'] for c in low_coherence_data[:3]])}")
        
        return low_coherence_data
    
    def generate_report(self):
        """G√©n√®re un rapport complet d'audit"""
        print("\n" + "="*60)
        print("üìã RAPPORT D'AUDIT COMPLET")
        print("="*60)
        
        # Charger les donn√©es
        self.load_data()
        
        # Analyses
        structure_info = self.analyze_structure()
        inconsistencies = self.detect_inconsistencies()
        semantic_results = self.evaluate_semantic_coherence(threshold=0.4)
        
        # R√©sum√©
        print("\n" + "="*60)
        print("üìå R√âSUM√â")
        print("="*60)
        
        total_issues = (
            len(inconsistencies['category_id_mismatch']) +
            len(inconsistencies['empty_paths']) +
            len(inconsistencies['invalid_paths'])
        )
        
        print(f"\n‚úÖ Points positifs:")
        print(f"   - {structure_info['unique_leaf_categories']:,} cat√©gories feuilles identifi√©es")
        print(f"   - Profondeur maximale: {structure_info['max_depth']} niveaux")
        
        if total_issues > 0:
            print(f"\n‚ö†Ô∏è  Points d'attention:")
            print(f"   - {total_issues:,} incoh√©rences structurelles d√©tect√©es")
            if len(inconsistencies['category_id_mismatch']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['category_id_mismatch'])} mismatches category_id/path")
            if len(inconsistencies['empty_paths']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['empty_paths'])} paths vides")
            if len(inconsistencies['invalid_paths']) > 0:
                print(f"     ‚Ä¢ {len(inconsistencies['invalid_paths'])} paths avec format invalide")
        else:
            print(f"\n‚úÖ Aucune incoh√©rence structurelle majeure d√©tect√©e")
        
        if semantic_results:
            print(f"   - {len(semantic_results)} cat√©gories avec faible coh√©rence s√©mantique (< 0.4)")
        
        print("\n" + "="*60)
        print("‚úì Audit termin√©")
        print("="*60)


def main():
    """Point d'entr√©e principal"""
    data_path = Path(__file__).parent / 'data' / 'trainset.csv'
    
    if not data_path.exists():
        print(f"‚ùå Fichier non trouv√©: {data_path}")
        return
    
    auditor = TaxonomyAuditor(data_path)
    auditor.generate_report()


if __name__ == '__main__':
    main()

