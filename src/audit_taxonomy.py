"""
√âtape 1 - Audit de la taxonomie existante

Ce script analyse le jeu de donn√©es d'entra√Ænement pour :
1. Comprendre la structure de la taxonomie (profondeur, nombre de cat√©gories par niveau)
2. D√©tecter des incoh√©rences structurelles dans les category_path
3. √âvaluer la coh√©rence s√©mantique des produits au sein de chaque cat√©gorie
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from pathlib import Path
import json
import re
import warnings
warnings.filterwarnings('ignore')

# Pour les embeddings s√©mantiques
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("Sentence-transformers non install√©. L'analyse s√©mantique sera limit√©e.")


class TaxonomyAuditor:
    """Auditeur de taxonomie pour produits e-commerce"""
    
    def __init__(self, data_path, sample_size=None):
        """
        Parameters:
        -----------
        data_path : str
            Chemin vers le fichier CSV d'entra√Ænement
        sample_size : int, optional
            Nombre d'√©chantillons √† utiliser pour l'analyse s√©mantique (None = tout)
        """
        self.data_path = data_path
        self.sample_size = sample_size
        self.df = None
        self.embedding_model = None
        
    def load_data(self):
        """Charge les donn√©es d'entra√Ænement"""
        print("Chargement des donn√©es...")
        self.df = pd.read_csv(self.data_path)
        print(f"   ‚úì {len(self.df):,} produits charg√©s")
        return self.df
    
    def analyze_structure(self):
        """Analyse la structure de la taxonomie"""
        print("\n" + "="*60)
        print("1 ANALYSE DE LA STRUCTURE DE LA TAXONOMIE")
        print("="*60)
        
        # Profondeur des chemins
        self.df['path_depth'] = self.df['category_path'].str.count('/') + 1
        depths = self.df['path_depth'].value_counts().sort_index()
        
        print(f"\nProfondeur de la taxonomie:")
        print(f"   Profondeur minimale: {depths.index.min()}")
        print(f"   Profondeur maximale: {depths.index.max()}")
        print(f"   Profondeur moyenne: {self.df['path_depth'].mean():.2f}")
        print(f"   Profondeur m√©diane: {self.df['path_depth'].median():.0f}")
        
        print(f"\nDistribution des profondeurs:")
        for depth, count in depths.items():
            pct = (count / len(self.df)) * 100
            print(f"   Niveau {depth}: {count:,} produits ({pct:.1f}%)")
        
        # Nombre de cat√©gories uniques par niveau
        print(f"\nNombre de cat√©gories uniques par niveau:")
        max_depth = int(self.df['path_depth'].max())
        for level in range(1, max_depth + 1):
            categories_at_level = set()
            for path in self.df['category_path']:
                parts = path.split('/')
                if len(parts) >= level:
                    categories_at_level.add(parts[level - 1])
            print(f"   Niveau {level}: {len(categories_at_level):,} cat√©gories uniques")
        
        # Cat√©gories feuilles
        unique_leaf_categories = self.df['category_id'].nunique()
        print(f"\nCat√©gories feuilles (category_id):")
        print(f"   {unique_leaf_categories:,} cat√©gories feuilles uniques")
        print(f"   {len(self.df):,} produits au total")
        avg_products_per_category = len(self.df) / unique_leaf_categories
        print(f"   Moyenne: {avg_products_per_category:.1f} produits par cat√©gorie")
        
        # Distribution des produits par cat√©gorie
        category_counts = self.df['category_id'].value_counts()
        print(f"\nDistribution des produits par cat√©gorie:")
        print(f"   Cat√©gorie la plus fr√©quente: {category_counts.max():,} produits")
        print(f"   Cat√©gorie la moins fr√©quente: {category_counts.min():,} produits")
        print(f"   M√©diane: {category_counts.median():.0f} produits")
        
        # Top 5 cat√©gories les plus fr√©quentes
        print(f"\nTop 5 cat√©gories les plus fr√©quentes:")
        top_5 = category_counts.head(5)
        for cat_id, count in top_5.items():
            print(f"   {cat_id}: {count:,} produits")
        
        # Bottom 5 cat√©gories les moins fr√©quentes
        print(f"\nTop 5 cat√©gories les moins fr√©quentes:")
        bottom_5 = category_counts.tail(5)
        for cat_id, count in bottom_5.items():
            print(f"   {cat_id}: {count:,} produits")
        
        # G√©n√©rer les noms de cat√©gories
        category_names = self.generate_category_names()
        
        return {
            'depths': depths,
            'max_depth': max_depth,
            'unique_leaf_categories': unique_leaf_categories,
            'category_counts': category_counts,
            'category_names': category_names
        }
    
    def generate_category_names(self):
        """G√©n√®re des noms simples pour chaque cat√©gorie bas√©s sur les mots-cl√©s fr√©quents"""
        print("\nG√©n√©ration des noms de cat√©gories...")
        
        # Stopwords simples (FR/DE/EN)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'et', 'ou', 'pour', 'avec', 'sans', 
                    'der', 'die', 'das', 'und', 'oder', 'f√ºr', 'mit', 'ohne',
                    'the', 'a', 'an', 'and', 'or', 'for', 'with', 'without',
                    '√†', 'd', 'l', 'un', 'une', 'en', 'sur', 'par', 'dans'}
        
        category_data = {}
        
        for cat_id in self.df['category_id'].unique():
            cat_products = self.df[self.df['category_id'] == cat_id]
            titles = cat_products['title'].fillna('').astype(str).tolist()
            
            # Extraire les mots (min 3 caract√®res, alphanum√©riques)
            words = []
            for title in titles:
                title_words = re.findall(r'\b[a-zA-Z√Ä-√ø]{3,}\b', title.lower())
                words.extend([w for w in title_words if w not in stopwords])
            
            # Top 2-3 mots les plus fr√©quents
            if words:
                word_counts = Counter(words)
                top_words = [word for word, _ in word_counts.most_common(3)]
                category_name = ' '.join(top_words).title()
            else:
                category_name = "Cat√©gorie inconnue"
            
            # Exemples de titres (max 5)
            example_titles = [t[:80] for t in titles[:5] if t.strip()]
            
            category_data[cat_id] = {
                'name': category_name,
                'example_titles': example_titles
            }
        
        # Sauvegarder
        output_path = Path(__file__).parent.parent / 'results' / 'audit' / 'category_names.json'
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(category_data, f, indent=2, ensure_ascii=False)
        
        print(f"   ‚úì {len(category_data)} noms g√©n√©r√©s")
        print(f"Sauvegard√©s dans: {output_path}")
        
        return category_data
    
    def load_category_names(self):
        """Charge les noms de cat√©gories depuis category_names.json"""
        names_path = Path(__file__).parent.parent / 'results' / 'audit' / 'category_names.json'
        if names_path.exists():
            with open(names_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def detect_inconsistencies(self):
        """D√©tecte les incoh√©rences structurelles dans les category_path"""
        print("\n" + "="*60)
        print("2 D√âTECTION D'INCOH√âRENCES STRUCTURELLES")
        print("="*60)
        
        inconsistencies = {
            'category_id_mismatch': [],
            'empty_paths': [],
            'invalid_paths': []
        }
        
        # V√©rifier que category_id correspond au dernier √©l√©ment du path
        print("\nV√©rification de coh√©rence category_id / category_path...")
        mismatches = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            last_path_id = path_parts[-1] if path_parts else None
            if last_path_id != row['category_id']:
                mismatches += 1
                inconsistencies['category_id_mismatch'].append({
                    'product_id': row['product_id'],
                    'category_id': row['category_id'],
                    'last_path_id': last_path_id,
                    'category_path': row['category_path']
                })
        
        if mismatches > 0:
            print(f"{mismatches:,} incoh√©rences d√©tect√©es (category_id ‚â† dernier √©l√©ment du path)")
            print(f"   Exemples (premiers 5):")
            for inc in inconsistencies['category_id_mismatch'][:5]:
                print(f"      - Product: {inc['product_id'][:8]}... | category_id: {inc['category_id']} | path fin: {inc['last_path_id']}")
        else:
            print(f"   ‚úì Aucune incoh√©rence d√©tect√©e")
        
        # V√©rifier les paths vides ou invalides
        print("\nV√©rification des paths vides ou invalides...")
        empty_paths = self.df[self.df['category_path'].isna() | (self.df['category_path'] == '')]
        if len(empty_paths) > 0:
            print(f"{len(empty_paths):,} produits avec path vide")
            inconsistencies['empty_paths'] = empty_paths['product_id'].tolist()
        else:
            print(f"   ‚úì Aucun path vide")
        
        # V√©rifier les paths avec des IDs invalides (format hexad√©cimal attendu)
        print("\nV√©rification du format des IDs dans les paths...")
        invalid_format = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for part in path_parts:
                # V√©rifier que c'est un hexad√©cimal de 8 caract√®res
                if len(part) != 8 or not all(c in '0123456789abcdef' for c in part.lower()):
                    invalid_format += 1
                    inconsistencies['invalid_paths'].append({
                        'product_id': row['product_id'],
                        'category_path': row['category_path'],
                        'invalid_part': part
                    })
                    break
        
        if invalid_format > 0:
            print(f"{invalid_format:,} paths avec format d'ID invalide")
        else:
            print(f"   ‚úì Tous les IDs ont un format valide (8 caract√®res hex)")
        
        # V√©rifier les chemins avec des doublons cons√©cutifs
        print("\nV√©rification des doublons cons√©cutifs dans les paths...")
        consecutive_duplicates = 0
        for idx, row in self.df.iterrows():
            path_parts = row['category_path'].split('/')
            for i in range(len(path_parts) - 1):
                if path_parts[i] == path_parts[i + 1]:
                    consecutive_duplicates += 1
                    break
        
        if consecutive_duplicates > 0:
            print(f"{consecutive_duplicates:,} paths avec doublons cons√©cutifs")
        else:
            print(f"   ‚úì Aucun doublon cons√©cutif")
        
        return inconsistencies
    
    def evaluate_semantic_coherence(self, threshold=0.4, min_products=10):
        """Analyse s√©mantique : √©value la coh√©rence et sauvegarde les cat√©gories probl√©matiques et performantes"""
        print("\n" + "="*60)
        print("3 √âVALUATION DE LA COH√âRENCE S√âMANTIQUE")
        print("="*60)
        
        if not EMBEDDINGS_AVAILABLE:
            print("\nSentence-transformers non disponible.")
            return None
        
        print("\nChargement du mod√®le d'embeddings...")
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Charger les noms de cat√©gories
        category_names = self.load_category_names()
        
        # Analyser toutes les cat√©gories avec suffisamment de produits
        category_counts = self.df['category_id'].value_counts()
        valid_categories = category_counts[category_counts >= min_products].index
        
        print(f"\nAnalyse de {len(valid_categories)} cat√©gories...")
        
        low_coherence_data = []
        high_coherence_data = []
        texts_combined = (self.df['title'].fillna('') + ' ' + self.df['description'].fillna('')).str.strip()
        
        for cat_id in valid_categories:
            cat_products = self.df[self.df['category_id'] == cat_id]
            if len(cat_products) > 100:
                cat_products = cat_products.sample(n=100, random_state=42)
            
            texts = texts_combined[cat_products.index].tolist()
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            
            # Distance moyenne intra-classe
            from sklearn.metrics.pairwise import cosine_distances
            distances = cosine_distances(embeddings)
            np.fill_diagonal(distances, np.nan)
            avg_distance = np.nanmean(distances)
            coherence_score = 1 - avg_distance
            
            # R√©cup√©rer quelques exemples de titres
            example_titles = [
                str(title)[:80] for title in cat_products['title'].head(5).fillna('')
                if str(title).strip()
            ]
            
            category_info = {
                'category_id': cat_id,
                'category_name': category_names.get(cat_id, {}).get('name', 'N/A') if isinstance(category_names.get(cat_id), dict) else category_names.get(cat_id, 'N/A'),
                'category_path': cat_products.iloc[0]['category_path'],
                'n_products': len(cat_products),
                'coherence_score': round(float(coherence_score), 3),
                'example_titles': example_titles[:5]
            }
            
            if coherence_score < threshold:
                low_coherence_data.append(category_info)
            else:
                high_coherence_data.append(category_info)
        
        # Trier : low par score croissant, high par score d√©croissant
        low_coherence_data.sort(key=lambda x: x['coherence_score'])
        high_coherence_data.sort(key=lambda x: x['coherence_score'], reverse=True)
        
        # Sauvegarder low_coherence_categories.json
        output_path_low = Path(__file__).parent.parent / 'results' / 'audit' / 'low_coherence_categories.json'
        output_path_low.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path_low, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': threshold,
                'total_low_coherence_categories': len(low_coherence_data),
                'categories': low_coherence_data
            }, f, indent=2, ensure_ascii=False)
        
        # Sauvegarder high_coherence_categories.json
        output_path_high = Path(__file__).parent.parent / 'results' / 'audit' / 'high_coherence_categories.json'
        with open(output_path_high, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': threshold,
                'total_high_coherence_categories': len(high_coherence_data),
                'categories': high_coherence_data
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ {len(low_coherence_data)} cat√©gories √† faible coh√©rence sauvegard√©es")
        if low_coherence_data:
            print(f"   Top 3: {', '.join([c['category_id'] for c in low_coherence_data[:3]])}")
        
        print(f"\nüíæ {len(high_coherence_data)} cat√©gories √† haute coh√©rence sauvegard√©es")
        if high_coherence_data:
            print(f"   Top 3: {', '.join([c['category_id'] for c in high_coherence_data[:3]])}")
        
        return low_coherence_data, high_coherence_data
    
    def check_train_test_distribution(self, test_path):
        """V√©rifie la coh√©rence des distributions train/test"""
        print("\n" + "="*60)
        print("4 V√âRIFICATION DISTRIBUTION TRAIN/TEST")
        print("="*60)
        
        df_test = pd.read_csv(test_path)
        
        # Calculer les proportions par cat√©gorie
        train_props = self.df['category_id'].value_counts(normalize=True).sort_index()
        test_props = df_test['category_id'].value_counts(normalize=True).sort_index()
        
        # V√©rifier les cat√©gories manquantes
        missing_in_test = set(train_props.index) - set(test_props.index)
        missing_in_train = set(test_props.index) - set(train_props.index)
        
        if missing_in_train:
            print(f"\n‚ö†Ô∏è  {len(missing_in_train)} cat√©gories du test absentes du train")
        
        # Aligner les index pour la corr√©lation
        common_cats = sorted(set(train_props.index) & set(test_props.index))
        train_aligned = train_props[common_cats]
        test_aligned = test_props[common_cats]
        
        # Corr√©lation et diff√©rence moyenne
        correlation = np.corrcoef(train_aligned, test_aligned)[0, 1]
        avg_diff = np.mean(np.abs(train_aligned - test_aligned)) * 100
        
        print(f"\nDistribution Train vs Test:")
        print(f"   ‚úì Corr√©lation: {correlation:.3f}")
        print(f"   ‚úì Diff√©rence moyenne: {avg_diff:.2f} points")
        
        # Cat√©gories avec √©cart > 2 points
        diffs = (train_aligned - test_aligned).abs() * 100
        large_diffs = diffs[diffs > 2]
        if len(large_diffs) > 0:
            print(f"   ‚ö†Ô∏è  {len(large_diffs)} cat√©gories avec √©cart > 2 points:")
            for cat_id, diff in large_diffs.head(5).items():
                print(f"      - {cat_id}: train {train_aligned[cat_id]*100:.1f}% vs test {test_aligned[cat_id]*100:.1f}% (diff: {diff:.1f})")
        else:
            print(f"   ‚úì Toutes les cat√©gories ont une distribution similaire")
    
    def generate_report(self, test_path=None):
        """G√©n√®re un rapport complet d'audit"""
        print("\n" + "="*60)
        print("RAPPORT D'AUDIT COMPLET")
        print("="*60)
        
        # Charger les donn√©es
        self.load_data()
        
        # Analyses
        self.analyze_structure()
        self.detect_inconsistencies()
        self.evaluate_semantic_coherence(threshold=0.4)
        
        # V√©rification train/test si test_path fourni
        if test_path:
            test_full_path = Path(__file__).parent.parent / test_path if not Path(test_path).is_absolute() else Path(test_path)
            if test_full_path.exists():
                self.check_train_test_distribution(test_full_path)
            else:
                print(f"\n‚ö†Ô∏è  Fichier test non trouv√©: {test_full_path}")
        
        print("\n" + "="*60)
        print("‚úì Audit termin√©")
        print("="*60)


def main():
    """Point d'entr√©e principal"""
    base_path = Path(__file__).parent.parent
    train_path = base_path / 'data' / 'trainset.csv'
    test_path = base_path / 'data' / 'testset.csv'
    
    if not train_path.exists():
        print(f"Fichier non trouv√©: {train_path}")
        return
    
    auditor = TaxonomyAuditor(train_path)
    auditor.generate_report(test_path='data/testset.csv' if test_path.exists() else None)


if __name__ == '__main__':
    main()

